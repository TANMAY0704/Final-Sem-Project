{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import talib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import yfinance as yf\n",
    "import talib\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stock_data(ticker):\n",
    "    # Fetch all available historical data\n",
    "    data = yf.download(ticker, progress=False)\n",
    "    \n",
    "    # Ensure the Close column is a 1D array before passing to TA-Lib\n",
    "    close_prices = data['Close'].astype(float).values.flatten()\n",
    "\n",
    "    data['RSI'] = talib.RSI(close_prices, timeperiod=14)\n",
    "    data['MA_10'] = talib.SMA(close_prices, timeperiod=10)\n",
    "    data['MA_30'] = talib.SMA(close_prices, timeperiod=30)\n",
    "    data['MA_50'] = talib.SMA(close_prices, timeperiod=50)\n",
    "    data['MA_200'] = talib.SMA(close_prices, timeperiod=200)\n",
    "\n",
    "    upper, middle, lower = talib.BBANDS(close_prices, timeperiod=20)\n",
    "    data['Upper_Band'] = upper\n",
    "    data['Lower_Band'] = lower\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "# Example Usage\n",
    " # Display the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lstm_data(df, columns=['Close'], time_step=60):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(df[columns])\n",
    "    X, y = [], []\n",
    "    for i in range(time_step, len(scaled_data)):\n",
    "        X.append(scaled_data[i-time_step:i])\n",
    "        y.append(scaled_data[i, 0])\n",
    "    return np.array(X), np.array(y), scaler\n",
    "\n",
    "# Build LSTM Model with Optimizations\n",
    "def build_stacked_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(X_train, y_train,X_test, y_test ,model_name,epoch):\n",
    "    model = build_stacked_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=3.1250e-05, verbose=1)\n",
    "    history = model.fit(X_train, y_train, epochs=epoch, batch_size=32,validation_data=(X_test, y_test), verbose=1, callbacks=[lr_scheduler])\n",
    "    model.save(f\"preduction_forcaste.keras\")\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "# Sentiment Analysis\n",
    "def get_sentiment_score(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    score = sia.polarity_scores(text)['compound']\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def forecast_ensemble(ticker, forecast_days=30, epoch=50):\n",
    "    df = get_stock_data(ticker)\n",
    "    time_step = 60\n",
    "    \n",
    "    # Ensure the correct columns are selected for feature engineering (Only 'Close' for indices)\n",
    "    feature_columns = ['Close']\n",
    "    \n",
    "    # Preprocess data\n",
    "    X, y, scaler = preprocess_lstm_data(df, columns=feature_columns, time_step=time_step)\n",
    "    X_train, y_train = X[:-forecast_days], y[:-forecast_days]\n",
    "    X_test, y_test = X[-forecast_days:], y[-forecast_days:]\n",
    "\n",
    "    try:\n",
    "        model = load_model(f\"preduction_forcaste.keras\")\n",
    "    except:\n",
    "        model = train_lstm_model(X_train, y_train, X_test, y_test, ticker, epoch)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Create an empty array for inverse transform with correct shape\n",
    "    dummy_array = np.zeros((predictions.shape[0], len(feature_columns)))  # (30, 1)\n",
    "    dummy_array[:, 0] = predictions[:, 0]  # Fill only the Close column\n",
    "\n",
    "    # Inverse transform using the full shape\n",
    "    predictions = scaler.inverse_transform(dummy_array)[:, 0]  # Extract only the Close column\n",
    "\n",
    "    # Inverse transform actual y values\n",
    "    y_actual = np.zeros((y_test.shape[0], len(feature_columns)))\n",
    "    y_actual[:, 0] = y_test  # Fill only the Close column\n",
    "    y_actual = scaler.inverse_transform(y_actual)[:, 0]  # Extract only Close column\n",
    "\n",
    "    # Sentiment Adjustment\n",
    "    headline = f\"{ticker} stock market update\"\n",
    "    sentiment_score = get_sentiment_score(headline)\n",
    "    sentiment_adjustment = 1 + (sentiment_score * 0.03)\n",
    "    adjusted_preds = predictions * sentiment_adjustment\n",
    "\n",
    "    # Calculate Evaluation Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, adjusted_preds))\n",
    "    mae = mean_absolute_error(y_actual, adjusted_preds)\n",
    "    mape = np.mean(np.abs((y_actual - adjusted_preds) / y_actual)) * 100\n",
    "    r2 = r2_score(y_actual, adjusted_preds)\n",
    "\n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"MAPE:\", mape)\n",
    "    print(\"R^2 Score:\", r2)\n",
    "\n",
    "    # Plot Results\n",
    "    last_dates = df.index[-forecast_days:]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(last_dates, y_actual, label='Actual')\n",
    "    \n",
    "    plt.plot(last_dates, adjusted_preds, label='Ensemble Forecast (Sentiment)', linestyle='--')\n",
    "    plt.title(f\"{ticker} - Enhanced Ensemble Forecast ({forecast_days} days), {epoch} Epochs\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MA_10</th>\n",
       "      <th>MA_30</th>\n",
       "      <th>MA_50</th>\n",
       "      <th>MA_200</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>RELIANCE.NS</th>\n",
       "      <th>RELIANCE.NS</th>\n",
       "      <th>RELIANCE.NS</th>\n",
       "      <th>RELIANCE.NS</th>\n",
       "      <th>RELIANCE.NS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-10-04</th>\n",
       "      <td>3.448852</td>\n",
       "      <td>3.476885</td>\n",
       "      <td>3.368373</td>\n",
       "      <td>3.449757</td>\n",
       "      <td>180091079</td>\n",
       "      <td>39.823197</td>\n",
       "      <td>3.476161</td>\n",
       "      <td>3.634709</td>\n",
       "      <td>3.682388</td>\n",
       "      <td>3.700528</td>\n",
       "      <td>3.757615</td>\n",
       "      <td>3.347981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-10-07</th>\n",
       "      <td>3.381938</td>\n",
       "      <td>3.456992</td>\n",
       "      <td>3.264384</td>\n",
       "      <td>3.456992</td>\n",
       "      <td>272128046</td>\n",
       "      <td>36.037561</td>\n",
       "      <td>3.462959</td>\n",
       "      <td>3.621296</td>\n",
       "      <td>3.673273</td>\n",
       "      <td>3.700440</td>\n",
       "      <td>3.731014</td>\n",
       "      <td>3.337778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-10-08</th>\n",
       "      <td>3.396405</td>\n",
       "      <td>3.436192</td>\n",
       "      <td>3.283372</td>\n",
       "      <td>3.365660</td>\n",
       "      <td>196707309</td>\n",
       "      <td>37.422563</td>\n",
       "      <td>3.448491</td>\n",
       "      <td>3.609389</td>\n",
       "      <td>3.666472</td>\n",
       "      <td>3.700557</td>\n",
       "      <td>3.704961</td>\n",
       "      <td>3.331820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-10-09</th>\n",
       "      <td>3.440715</td>\n",
       "      <td>3.472364</td>\n",
       "      <td>3.386459</td>\n",
       "      <td>3.436193</td>\n",
       "      <td>175357589</td>\n",
       "      <td>41.594181</td>\n",
       "      <td>3.435922</td>\n",
       "      <td>3.595464</td>\n",
       "      <td>3.657954</td>\n",
       "      <td>3.700767</td>\n",
       "      <td>3.681073</td>\n",
       "      <td>3.332107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-10-10</th>\n",
       "      <td>3.532949</td>\n",
       "      <td>3.544704</td>\n",
       "      <td>3.414491</td>\n",
       "      <td>3.430768</td>\n",
       "      <td>194458201</td>\n",
       "      <td>49.187508</td>\n",
       "      <td>3.438182</td>\n",
       "      <td>3.585818</td>\n",
       "      <td>3.652692</td>\n",
       "      <td>3.701596</td>\n",
       "      <td>3.663105</td>\n",
       "      <td>3.338409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-28</th>\n",
       "      <td>1275.099976</td>\n",
       "      <td>1295.750000</td>\n",
       "      <td>1269.000000</td>\n",
       "      <td>1280.000000</td>\n",
       "      <td>18147129</td>\n",
       "      <td>57.700779</td>\n",
       "      <td>1268.424988</td>\n",
       "      <td>1234.701668</td>\n",
       "      <td>1245.083999</td>\n",
       "      <td>1360.224517</td>\n",
       "      <td>1319.400084</td>\n",
       "      <td>1164.914918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-01</th>\n",
       "      <td>1252.599976</td>\n",
       "      <td>1277.900024</td>\n",
       "      <td>1249.300049</td>\n",
       "      <td>1264.599976</td>\n",
       "      <td>12099648</td>\n",
       "      <td>50.443446</td>\n",
       "      <td>1269.799988</td>\n",
       "      <td>1235.918335</td>\n",
       "      <td>1244.088999</td>\n",
       "      <td>1359.195640</td>\n",
       "      <td>1319.661711</td>\n",
       "      <td>1169.903291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-02</th>\n",
       "      <td>1251.150024</td>\n",
       "      <td>1255.550049</td>\n",
       "      <td>1243.900024</td>\n",
       "      <td>1247.550049</td>\n",
       "      <td>10142590</td>\n",
       "      <td>50.006949</td>\n",
       "      <td>1271.029993</td>\n",
       "      <td>1237.048336</td>\n",
       "      <td>1243.003000</td>\n",
       "      <td>1358.149921</td>\n",
       "      <td>1315.633761</td>\n",
       "      <td>1181.921244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-03</th>\n",
       "      <td>1248.699951</td>\n",
       "      <td>1251.800049</td>\n",
       "      <td>1233.050049</td>\n",
       "      <td>1233.050049</td>\n",
       "      <td>7434366</td>\n",
       "      <td>49.231717</td>\n",
       "      <td>1271.184985</td>\n",
       "      <td>1237.841667</td>\n",
       "      <td>1242.503000</td>\n",
       "      <td>1357.030660</td>\n",
       "      <td>1306.828568</td>\n",
       "      <td>1199.406430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-04</th>\n",
       "      <td>1204.699951</td>\n",
       "      <td>1245.449951</td>\n",
       "      <td>1193.150024</td>\n",
       "      <td>1241.099976</td>\n",
       "      <td>17906607</td>\n",
       "      <td>37.875811</td>\n",
       "      <td>1264.739978</td>\n",
       "      <td>1237.151664</td>\n",
       "      <td>1241.055000</td>\n",
       "      <td>1355.674082</td>\n",
       "      <td>1300.870115</td>\n",
       "      <td>1208.274880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7148 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price             Close         High          Low         Open      Volume  \\\n",
       "Ticker      RELIANCE.NS  RELIANCE.NS  RELIANCE.NS  RELIANCE.NS RELIANCE.NS   \n",
       "Date                                                                         \n",
       "1996-10-04     3.448852     3.476885     3.368373     3.449757   180091079   \n",
       "1996-10-07     3.381938     3.456992     3.264384     3.456992   272128046   \n",
       "1996-10-08     3.396405     3.436192     3.283372     3.365660   196707309   \n",
       "1996-10-09     3.440715     3.472364     3.386459     3.436193   175357589   \n",
       "1996-10-10     3.532949     3.544704     3.414491     3.430768   194458201   \n",
       "...                 ...          ...          ...          ...         ...   \n",
       "2025-03-28  1275.099976  1295.750000  1269.000000  1280.000000    18147129   \n",
       "2025-04-01  1252.599976  1277.900024  1249.300049  1264.599976    12099648   \n",
       "2025-04-02  1251.150024  1255.550049  1243.900024  1247.550049    10142590   \n",
       "2025-04-03  1248.699951  1251.800049  1233.050049  1233.050049     7434366   \n",
       "2025-04-04  1204.699951  1245.449951  1193.150024  1241.099976    17906607   \n",
       "\n",
       "Price             RSI        MA_10        MA_30        MA_50       MA_200  \\\n",
       "Ticker                                                                      \n",
       "Date                                                                        \n",
       "1996-10-04  39.823197     3.476161     3.634709     3.682388     3.700528   \n",
       "1996-10-07  36.037561     3.462959     3.621296     3.673273     3.700440   \n",
       "1996-10-08  37.422563     3.448491     3.609389     3.666472     3.700557   \n",
       "1996-10-09  41.594181     3.435922     3.595464     3.657954     3.700767   \n",
       "1996-10-10  49.187508     3.438182     3.585818     3.652692     3.701596   \n",
       "...               ...          ...          ...          ...          ...   \n",
       "2025-03-28  57.700779  1268.424988  1234.701668  1245.083999  1360.224517   \n",
       "2025-04-01  50.443446  1269.799988  1235.918335  1244.088999  1359.195640   \n",
       "2025-04-02  50.006949  1271.029993  1237.048336  1243.003000  1358.149921   \n",
       "2025-04-03  49.231717  1271.184985  1237.841667  1242.503000  1357.030660   \n",
       "2025-04-04  37.875811  1264.739978  1237.151664  1241.055000  1355.674082   \n",
       "\n",
       "Price        Upper_Band   Lower_Band  \n",
       "Ticker                                \n",
       "Date                                  \n",
       "1996-10-04     3.757615     3.347981  \n",
       "1996-10-07     3.731014     3.337778  \n",
       "1996-10-08     3.704961     3.331820  \n",
       "1996-10-09     3.681073     3.332107  \n",
       "1996-10-10     3.663105     3.338409  \n",
       "...                 ...          ...  \n",
       "2025-03-28  1319.400084  1164.914918  \n",
       "2025-04-01  1319.661711  1169.903291  \n",
       "2025-04-02  1315.633761  1181.921244  \n",
       "2025-04-03  1306.828568  1199.406430  \n",
       "2025-04-04  1300.870115  1208.274880  \n",
       "\n",
       "[7148 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_stock_data(\"RELIANCE.NS\")  # Replace with your stock ticker\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pande\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 177ms/step - loss: 0.0313 - val_loss: 0.2347 - learning_rate: 0.0010\n",
      "Epoch 2/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 152ms/step - loss: 0.0039 - val_loss: 0.1131 - learning_rate: 0.0010\n",
      "Epoch 3/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 155ms/step - loss: 0.0020 - val_loss: 0.0096 - learning_rate: 0.0010\n",
      "Epoch 4/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 150ms/step - loss: 0.0015 - val_loss: 4.8885e-04 - learning_rate: 0.0010\n",
      "Epoch 5/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 153ms/step - loss: 0.0010 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 6/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 151ms/step - loss: 0.0011 - val_loss: 4.6898e-04 - learning_rate: 0.0010\n",
      "Epoch 7/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 144ms/step - loss: 8.8436e-04 - val_loss: 9.2163e-04 - learning_rate: 0.0010\n",
      "Epoch 8/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 151ms/step - loss: 6.6882e-04 - val_loss: 5.5527e-04 - learning_rate: 0.0010\n",
      "Epoch 9/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 156ms/step - loss: 7.7368e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 10/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 156ms/step - loss: 6.7901e-04 - val_loss: 4.7795e-04 - learning_rate: 0.0010\n",
      "Epoch 11/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 128ms/step - loss: 5.3305e-04 - val_loss: 4.3337e-04 - learning_rate: 0.0010\n",
      "Epoch 12/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 144ms/step - loss: 5.6570e-04 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 13/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 6.9926e-04\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 149ms/step - loss: 6.9934e-04 - val_loss: 0.0025 - learning_rate: 0.0010\n",
      "Epoch 14/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 155ms/step - loss: 7.4009e-04 - val_loss: 6.9767e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 15/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 149ms/step - loss: 4.3308e-04 - val_loss: 5.2524e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 16/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 157ms/step - loss: 4.0852e-04 - val_loss: 5.5120e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 17/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 151ms/step - loss: 4.1218e-04 - val_loss: 3.9805e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 18/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 147ms/step - loss: 4.4793e-04 - val_loss: 0.0024 - learning_rate: 5.0000e-04\n",
      "Epoch 19/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 118ms/step - loss: 4.6675e-04 - val_loss: 9.0561e-04 - learning_rate: 5.0000e-04\n",
      "Epoch 20/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 127ms/step - loss: 4.5261e-04 - val_loss: 0.0014 - learning_rate: 5.0000e-04\n",
      "Epoch 21/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 3.3394e-04\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 113ms/step - loss: 3.3400e-04 - val_loss: 0.0016 - learning_rate: 5.0000e-04\n",
      "Epoch 22/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 149ms/step - loss: 3.2608e-04 - val_loss: 5.2183e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 23/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 153ms/step - loss: 2.8228e-04 - val_loss: 4.8543e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 24/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 138ms/step - loss: 3.1508e-04 - val_loss: 3.9793e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 25/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 155ms/step - loss: 3.0272e-04 - val_loss: 3.5475e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 26/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 3.3252e-04\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 140ms/step - loss: 3.3252e-04 - val_loss: 0.0013 - learning_rate: 2.5000e-04\n",
      "Epoch 27/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 127ms/step - loss: 2.9002e-04 - val_loss: 7.7205e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 28/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 149ms/step - loss: 3.1296e-04 - val_loss: 4.0492e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 29/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 149ms/step - loss: 3.0953e-04 - val_loss: 3.5475e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 30/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 166ms/step - loss: 2.9157e-04 - val_loss: 0.0011 - learning_rate: 1.2500e-04\n",
      "Epoch 31/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 149ms/step - loss: 2.5231e-04 - val_loss: 4.4590e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 32/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 2.9040e-04\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 159ms/step - loss: 2.9031e-04 - val_loss: 3.4653e-04 - learning_rate: 1.2500e-04\n",
      "Epoch 33/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 150ms/step - loss: 2.4611e-04 - val_loss: 3.3217e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 34/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 178ms/step - loss: 2.2179e-04 - val_loss: 3.7648e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 35/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 182ms/step - loss: 2.3396e-04 - val_loss: 4.9521e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 36/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 153ms/step - loss: 2.2778e-04 - val_loss: 3.3722e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 37/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 2.7897e-04\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 166ms/step - loss: 2.7881e-04 - val_loss: 3.9513e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 38/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 161ms/step - loss: 2.2448e-04 - val_loss: 3.5669e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 39/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 131ms/step - loss: 2.3374e-04 - val_loss: 4.0647e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 40/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 127ms/step - loss: 2.1139e-04 - val_loss: 3.6030e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 41/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 161ms/step - loss: 2.1802e-04 - val_loss: 5.3613e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 42/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 150ms/step - loss: 2.0114e-04 - val_loss: 3.3014e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 43/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 162ms/step - loss: 2.0519e-04 - val_loss: 3.3084e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 44/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 159ms/step - loss: 2.0320e-04 - val_loss: 3.2117e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 45/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 156ms/step - loss: 2.0136e-04 - val_loss: 4.0312e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 46/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 162ms/step - loss: 1.9816e-04 - val_loss: 3.2847e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 47/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 163ms/step - loss: 2.0842e-04 - val_loss: 3.4422e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 48/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 169ms/step - loss: 1.9726e-04 - val_loss: 4.7970e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 49/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 154ms/step - loss: 1.9436e-04 - val_loss: 3.2434e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 50/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 2.3020e-04 - val_loss: 3.1595e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 51/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 138ms/step - loss: 1.9692e-04 - val_loss: 3.6577e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 52/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 124ms/step - loss: 1.9042e-04 - val_loss: 3.2928e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 53/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 142ms/step - loss: 1.8413e-04 - val_loss: 3.2275e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 54/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 159ms/step - loss: 1.7422e-04 - val_loss: 3.9265e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 55/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 160ms/step - loss: 1.7131e-04 - val_loss: 3.4284e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 56/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 167ms/step - loss: 1.9041e-04 - val_loss: 3.2268e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 57/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 165ms/step - loss: 1.7740e-04 - val_loss: 4.1762e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 58/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - loss: 1.7511e-04 - val_loss: 3.2836e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 59/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - loss: 1.6403e-04 - val_loss: 3.1686e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 60/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 158ms/step - loss: 1.6476e-04 - val_loss: 3.0443e-04 - learning_rate: 3.1250e-05\n",
      "Epoch 61/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 1.7048e-04"
     ]
    }
   ],
   "source": [
    "forecast_ensemble(\"RELIANCE.NS\",epoch=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pande\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\holidays\\countries\\india.py:176: Warning: Requested Holidays are available only from 2001 to 2035.\n",
      "  warnings.warn(warning_msg, Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 4 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Example Usage\u001b[39;00m\n\u001b[0;32m     47\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreduction_forcaste.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m forecast_df \u001b[38;5;241m=\u001b[39m forecast_beyond_data(model, scaler, last_known_data)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(forecast_df)\n",
      "Cell \u001b[1;32mIn[8], line 40\u001b[0m, in \u001b[0;36mforecast_beyond_data\u001b[1;34m(model, scaler, last_known_data, steps)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Update the input for the next prediction\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     scaled_prediction \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(np\u001b[38;5;241m.\u001b[39marray([[predicted_value]])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 40\u001b[0m     current_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(current_data[:, \u001b[38;5;241m1\u001b[39m:, :], [[scaled_prediction]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m     current_date \u001b[38;5;241m=\u001b[39m next_day\n\u001b[0;32m     43\u001b[0m forecast_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(predictions, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mForecasted_Close\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\pande\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:5711\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5709\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[0;32m   5710\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 5711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concatenate((arr, values), axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 4 dimension(s)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import holidays\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(r'C:\\GIT REPOS\\Final-Sem-Project\\DATA\\nse_stock_data\\RELIANCE.CSV', skiprows=2,\n",
    "                 names=['Date', 'Close', 'High', 'Low', 'Open', 'Volume'])\n",
    "\n",
    "# Initialize and fit the scaler on the last known data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "last_known_data = df[['Close']].values\n",
    "scaler.fit(last_known_data)  # Fit the scaler with the data\n",
    "\n",
    "# Get Indian holidays\n",
    "indian_holidays = holidays.India()\n",
    "\n",
    "def get_next_trading_day(current_date):\n",
    "    next_day = current_date + timedelta(days=1)\n",
    "    while next_day.weekday() >= 5 or next_day in indian_holidays:\n",
    "        next_day += timedelta(days=1)\n",
    "    return next_day\n",
    "\n",
    "def forecast_beyond_data(model, scaler, last_known_data, steps=30):\n",
    "    predictions = []\n",
    "    current_data = last_known_data[-60:].reshape(1, 60, 1)\n",
    "    current_date = pd.to_datetime(df.index[-1])\n",
    "\n",
    "    for _ in range(steps):\n",
    "        next_day = get_next_trading_day(current_date)\n",
    "        predicted_value = model.predict(current_data)[0, 0]\n",
    "        # Inverse transform using the fitted scaler\n",
    "        predicted_value = scaler.inverse_transform([[predicted_value]])[0, 0]\n",
    "        predictions.append((next_day.strftime('%Y-%m-%d'), predicted_value))\n",
    "\n",
    "        # Update the input for the next prediction\n",
    "        scaled_prediction = scaler.transform(np.array([[predicted_value]]).reshape(1, -1))\n",
    "        current_data = np.append(current_data[:, 1:, :], [[scaled_prediction]], axis=1)\n",
    "        current_date = next_day\n",
    "\n",
    "    forecast_df = pd.DataFrame(predictions, columns=['Date', 'Forecasted_Close'])\n",
    "    return forecast_df\n",
    "\n",
    "# Example Usage\n",
    "model = load_model(\"preduction_forcaste.keras\")\n",
    "forecast_df = forecast_beyond_data(model, scaler, last_known_data)\n",
    "print(forecast_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
