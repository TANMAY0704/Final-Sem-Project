{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import talib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import yfinance as yf\n",
    "import talib\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stock_data(ticker):\n",
    "    # Fetch all available historical data\n",
    "    data = yf.download(ticker, progress=False)\n",
    "    \n",
    "    # Ensure the Close column is a 1D array before passing to TA-Lib\n",
    "    close_prices = data['Close'].astype(float).values.flatten()\n",
    "\n",
    "    data['RSI'] = talib.RSI(close_prices, timeperiod=14)\n",
    "    data['MA_10'] = talib.SMA(close_prices, timeperiod=10)\n",
    "    data['MA_30'] = talib.SMA(close_prices, timeperiod=30)\n",
    "    data['MA_50'] = talib.SMA(close_prices, timeperiod=50)\n",
    "    data['MA_200'] = talib.SMA(close_prices, timeperiod=200)\n",
    "\n",
    "    upper, middle, lower = talib.BBANDS(close_prices, timeperiod=20)\n",
    "    data['Upper_Band'] = upper\n",
    "    data['Lower_Band'] = lower\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "# Example Usage\n",
    " # Display the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lstm_data(df, columns=['Close'], time_step=60):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(df[columns])\n",
    "    X, y = [], []\n",
    "    for i in range(time_step, len(scaled_data)):\n",
    "        X.append(scaled_data[i-time_step:i])\n",
    "        y.append(scaled_data[i, 0])\n",
    "    return np.array(X), np.array(y), scaler\n",
    "\n",
    "# Build LSTM Model with Optimizations\n",
    "def build_stacked_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(X_train, y_train, model_name,epoch):\n",
    "    model = build_stacked_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=3.1250e-05, verbose=1)\n",
    "    model.fit(X_train, y_train, epochs=epoch, batch_size=32, verbose=1, callbacks=[lr_scheduler])\n",
    "    model.save(f\"preduction_forcaste.keras\")\n",
    "    return model\n",
    "\n",
    "# Sentiment Analysis\n",
    "def get_sentiment_score(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    score = sia.polarity_scores(text)['compound']\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def forecast_ensemble(ticker, forecast_days=30, epoch=50):\n",
    "    df = get_stock_data(ticker)\n",
    "    time_step = 60\n",
    "    \n",
    "    # Ensure the correct columns are selected for feature engineering (Only 'Close' for indices)\n",
    "    feature_columns = ['Close']\n",
    "    \n",
    "    # Preprocess data\n",
    "    X, y, scaler = preprocess_lstm_data(df, columns=feature_columns, time_step=time_step)\n",
    "    X_train, y_train = X[:-forecast_days], y[:-forecast_days]\n",
    "    X_test = X[-forecast_days:]\n",
    "\n",
    "    try:\n",
    "        model = load_model(f\"preduction_forcaste.keras\")\n",
    "    except:\n",
    "        model = train_lstm_model(X_train, y_train, ticker, epoch)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Create an empty array for inverse transform with correct shape\n",
    "    dummy_array = np.zeros((predictions.shape[0], len(feature_columns)))  # (30, 1)\n",
    "    dummy_array[:, 0] = predictions[:, 0]  # Fill only the Close column\n",
    "\n",
    "    # Inverse transform using the full shape\n",
    "    predictions = scaler.inverse_transform(dummy_array)[:, 0]  # Extract only the Close column\n",
    "\n",
    "    # Inverse transform actual y values\n",
    "    y_actual = np.zeros((y[-forecast_days:].shape[0], len(feature_columns)))\n",
    "    y_actual[:, 0] = y[-forecast_days:]  # Fill only the Close column\n",
    "    y_actual = scaler.inverse_transform(y_actual)[:, 0]  # Extract only Close column\n",
    "\n",
    "    # Sentiment Adjustment\n",
    "    headline = f\"{ticker} stock market update\"\n",
    "    sentiment_score = get_sentiment_score(headline)\n",
    "    sentiment_adjustment = 1 + (sentiment_score * 0.03)\n",
    "    adjusted_preds = predictions * sentiment_adjustment\n",
    "\n",
    "    # Calculate Evaluation Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, adjusted_preds))\n",
    "    mae = mean_absolute_error(y_actual, adjusted_preds)\n",
    "    mape = np.mean(np.abs((y_actual - adjusted_preds) / y_actual)) * 100\n",
    "    r2 = r2_score(y_actual, adjusted_preds)\n",
    "\n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"MAPE:\", mape)\n",
    "    print(\"R^2 Score:\", r2)\n",
    "\n",
    "    # Plot Results\n",
    "    last_dates = df.index[-forecast_days:]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(last_dates, y_actual, label='Actual')\n",
    "    plt.plot(last_dates, predictions, label='LSTM Forecast')\n",
    "    plt.plot(last_dates, adjusted_preds, label='Ensemble Forecast (Sentiment)', linestyle='--')\n",
    "    plt.title(f\"{ticker} - Enhanced Ensemble Forecast ({forecast_days} days), {epoch} Epochs\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MA_10</th>\n",
       "      <th>MA_30</th>\n",
       "      <th>MA_50</th>\n",
       "      <th>MA_200</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>RELIANCE.NS</th>\n",
       "      <th>RELIANCE.NS</th>\n",
       "      <th>RELIANCE.NS</th>\n",
       "      <th>RELIANCE.NS</th>\n",
       "      <th>RELIANCE.NS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-10-04</th>\n",
       "      <td>3.514127</td>\n",
       "      <td>3.542690</td>\n",
       "      <td>3.432125</td>\n",
       "      <td>3.515048</td>\n",
       "      <td>180091079</td>\n",
       "      <td>39.809373</td>\n",
       "      <td>3.541952</td>\n",
       "      <td>3.703500</td>\n",
       "      <td>3.752081</td>\n",
       "      <td>3.860209</td>\n",
       "      <td>3.828731</td>\n",
       "      <td>3.411346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-10-07</th>\n",
       "      <td>3.445945</td>\n",
       "      <td>3.522419</td>\n",
       "      <td>3.326166</td>\n",
       "      <td>3.522419</td>\n",
       "      <td>272128046</td>\n",
       "      <td>36.024384</td>\n",
       "      <td>3.528500</td>\n",
       "      <td>3.689833</td>\n",
       "      <td>3.742793</td>\n",
       "      <td>3.859301</td>\n",
       "      <td>3.801627</td>\n",
       "      <td>3.400950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-10-08</th>\n",
       "      <td>3.460686</td>\n",
       "      <td>3.501226</td>\n",
       "      <td>3.345514</td>\n",
       "      <td>3.429359</td>\n",
       "      <td>196707309</td>\n",
       "      <td>37.409943</td>\n",
       "      <td>3.513758</td>\n",
       "      <td>3.677701</td>\n",
       "      <td>3.735865</td>\n",
       "      <td>3.858607</td>\n",
       "      <td>3.775082</td>\n",
       "      <td>3.394878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-10-09</th>\n",
       "      <td>3.505833</td>\n",
       "      <td>3.538081</td>\n",
       "      <td>3.450551</td>\n",
       "      <td>3.501226</td>\n",
       "      <td>175357589</td>\n",
       "      <td>41.582910</td>\n",
       "      <td>3.500951</td>\n",
       "      <td>3.663512</td>\n",
       "      <td>3.727185</td>\n",
       "      <td>3.858002</td>\n",
       "      <td>3.750741</td>\n",
       "      <td>3.395171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-10-10</th>\n",
       "      <td>3.599814</td>\n",
       "      <td>3.611792</td>\n",
       "      <td>3.479114</td>\n",
       "      <td>3.495699</td>\n",
       "      <td>194458201</td>\n",
       "      <td>49.178681</td>\n",
       "      <td>3.503254</td>\n",
       "      <td>3.653684</td>\n",
       "      <td>3.721823</td>\n",
       "      <td>3.858035</td>\n",
       "      <td>3.732434</td>\n",
       "      <td>3.401593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price            Close        High         Low        Open      Volume  \\\n",
       "Ticker     RELIANCE.NS RELIANCE.NS RELIANCE.NS RELIANCE.NS RELIANCE.NS   \n",
       "Date                                                                     \n",
       "1996-10-04    3.514127    3.542690    3.432125    3.515048   180091079   \n",
       "1996-10-07    3.445945    3.522419    3.326166    3.522419   272128046   \n",
       "1996-10-08    3.460686    3.501226    3.345514    3.429359   196707309   \n",
       "1996-10-09    3.505833    3.538081    3.450551    3.501226   175357589   \n",
       "1996-10-10    3.599814    3.611792    3.479114    3.495699   194458201   \n",
       "\n",
       "Price             RSI     MA_10     MA_30     MA_50    MA_200 Upper_Band  \\\n",
       "Ticker                                                                     \n",
       "Date                                                                       \n",
       "1996-10-04  39.809373  3.541952  3.703500  3.752081  3.860209   3.828731   \n",
       "1996-10-07  36.024384  3.528500  3.689833  3.742793  3.859301   3.801627   \n",
       "1996-10-08  37.409943  3.513758  3.677701  3.735865  3.858607   3.775082   \n",
       "1996-10-09  41.582910  3.500951  3.663512  3.727185  3.858002   3.750741   \n",
       "1996-10-10  49.178681  3.503254  3.653684  3.721823  3.858035   3.732434   \n",
       "\n",
       "Price      Lower_Band  \n",
       "Ticker                 \n",
       "Date                   \n",
       "1996-10-04   3.411346  \n",
       "1996-10-07   3.400950  \n",
       "1996-10-08   3.394878  \n",
       "1996-10-09   3.395171  \n",
       "1996-10-10   3.401593  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_stock_data(\"RELIANCE.NS\")  # Replace with your stock ticker\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pande\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 62ms/step - loss: 0.0181 - learning_rate: 0.0010\n",
      "Epoch 2/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 70ms/step - loss: 0.0027 - learning_rate: 0.0010\n",
      "Epoch 3/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 80ms/step - loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 4/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 89ms/step - loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 5/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 80ms/step - loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 6/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 77ms/step - loss: 8.8737e-04 - learning_rate: 0.0010\n",
      "Epoch 7/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 74ms/step - loss: 7.1087e-04 - learning_rate: 0.0010\n",
      "Epoch 8/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 74ms/step - loss: 8.7633e-04 - learning_rate: 0.0010\n",
      "Epoch 9/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 84ms/step - loss: 7.5864e-04 - learning_rate: 0.0010\n",
      "Epoch 10/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 80ms/step - loss: 7.4012e-04 - learning_rate: 0.0010\n",
      "Epoch 11/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 80ms/step - loss: 6.8343e-04 - learning_rate: 0.0010\n",
      "Epoch 12/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 81ms/step - loss: 5.4713e-04 - learning_rate: 0.0010\n",
      "Epoch 13/250\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 71ms/step - loss: 6.0358e-04 - learning_rate: 0.0010\n",
      "Epoch 14/250\n",
      "\u0